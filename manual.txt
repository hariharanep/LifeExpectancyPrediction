Instructions on How to Run Code
1. Obtaining Preprocessed Dataset
In the sources folder start python from the terminal and type these commands
>>> import preprocessing
>>> data, colnames, labels = preprocessing.run("../dataset/lifeexpecdata.csv")
data is a multidimensional numpy array representing the preprocessed dataset.
colnames is a numpy array with the column names for each feature in data.
labels is a one-dimensional numpy array with the classifcation labels for each sample.

2. K-Nearest Neighbors Hyperparameter Tuning with Training/Validation/Testing
In the sources folder start python from the terminal and type these commands
>>> import preprocessing
>>> data, colnames, labels = preprocessing.run("../dataset/lifeexpecdata.csv")
>>> import k_nearest_neighbors
>>> k_list, errors = k_nearest_neighbors.training_validation_testing(data, labels) 
>>> import hyperparametervserror
>>> hyperparametervserror.graph(k_list, errors)
The first four lines execute the training/validation/testing for k-nearest neighbors which
returns the list of k-values that were tested and their errors on the validation set.
The last two lines display a graph of each k-value tested and its error on the validation 
set in training/validation/testing.

3. K-Nearest Neighbors Hyperparameter Tuning with Bootstrapping
In the sources folder start python from the terminal and type these commands
>>> import preprocessing
>>> data, colnames, labels = preprocessing.run("../dataset/lifeexpecdata.csv")
>>> import k_nearest_neighbors
>>> k_list, errors = k_nearest_neighbors.bootstrapping(50, data, labels)
>>> import hyperparametervserror
>>> hyperparametervserror.graph(k_list, errors)
The first four lines execute our bootstrapping algorithm for k-nearest neighbors which
returns the list of k-values that were tested and their average errors for all 50 of their
bootstrapping iterations. It takes around a minute and 20 seconds for the 
k_nearest_neighbors.bootstrapping(50, data, labels) function to finish execution.
The last two lines display a graph of each k-value tested and its average error for all 
50 of its bootstrapping iterations.

4. Naive-Bayes Hyperparameter Tuning with Training/Validation/Testing
In the sources folder start python from the terminal and type these commands
>>> import preprocessing
>>> data, colnames, labels = preprocessing.run("../dataset/lifeexpecdata.csv")
>>> import naive_bayes
>>> naive_bayes.training_validation_testing(data, labels)
These four lines execute the training/validation/testing for Naive-Bayes which just
prints all the Naive-Bayes algorithms that were tested and their errors on the 
validation set. It also prints out the Naive-Bayes algorithm with the lowest error on the
validation set and its resultant error on the test set.

5. Naive-Bayes Hyperparameter Tuning with Bootstrapping
In the sources folder start python from the terminal and type these commands
>>> import preprocessing
>>> data, colnames, labels = preprocessing.run("../dataset/lifeexpecdata.csv")
>>> import naive_bayes
>>> naive_bayes.bootstrapping(50, data, labels)
These four lines execute our bootstrapping algorithm for Naive-Bayes which just prints
the average errors for all 50 bootstrapping iterations of each Naive-Bayes algorithm that
we considered. It also prints out the Naive-Bayes algorithm with the lowest average error
for all 50 of its bootstrapping iterations.

6. Feature Selection
In the sources folder start python from the terminal and type these commands
>>> import preprocessing
>>> data, colnames, labels = preprocessing.run("../dataset/lifeexpecdata.csv")
>>> import featureselection
>>> featureselection.generate_explained_variance_graph(data)
>>> featureselection.one_feature_classification_results(data, labels, 16)
These first four lines generate a graph that shows the total share of the explained variance
in the dataset for different numbers of components. 
The last line generates the printed output for K-Nearest Neighbors and Naive-Bayes 
training/validation/testing on a reduced version of the original dataset that only
includes the Population feature.

7. ROC Curves
In the sources folder start python from the terminal and type these commands
>>> import preprocessing
>>> data, colnames, labels = preprocessing.run("../dataset/lifeexpecdata.csv")
>>> import roc_curve
>>> roc_curve.k_nearest_neighbors(data, labels)
>>> roc_curve.naive_bayes(data, labels)
These first four lines generate three graphs showing a ROC curve for the
k-nearest neighbors algorithm by varying the hyperparameter k for each classification
label in our classification system.
The last line generates three graphs showing a ROC curve for the Naive-Bayes 
algorithm by varying the hyperparameter(type of prior distribution the Naive-Bayes 
algorithm assumes) for each classification label in our classification system.


Libraries Used
1. numpy
2. matplotlib
3. sklearn
4. csv

You don't need to download the csv module since it's already included in the installation
of Python. You can download all the other libraries with the command
"pip3 install <library name>" assuming that the version of Python installed is Python 3.
If the version of Python installed is Python 2 you can download all these libraries with
the command "pip install <library name>".
